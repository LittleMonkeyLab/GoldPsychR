{
  "hash": "3fff9ce35effebc595fe9a476a94137d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Boil in the Bag: Simple Linear Regression\"\nsubtitle: \"One predictor, one outcome\"\nformat: html\n---\n\n## Overview\n\n**Use this template when:** You want to predict a continuous outcome from a single continuous predictor.\n\n**Example scenarios:**\n- Predicting exam score from study hours\n- Predicting wellbeing from social media use\n- Predicting anxiety from personality score\n\n\n## Step 1: Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(effectsize)\n```\n:::\n\n\n\n## Step 2: Load Your Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THIS: Replace with your data file\ndata <- read_csv(\"data/simple_regression_data.csv\")\n\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 25\nColumns: 5\n$ id        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       <dbl> 21, 22, 20, 23, 21, 22, 20, 24, 21, 22, 23, 20, 21, 22, 20, …\n$ gender    <dbl> 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, …\n$ predictor <dbl> 12, 15, 18, 14, 20, 16, 22, 13, 19, 17, 21, 11, 24, 10, 23, …\n$ outcome   <dbl> 45, 52, 58, 48, 62, 54, 68, 46, 60, 56, 65, 42, 72, 40, 70, …\n```\n\n\n:::\n:::\n\n\n\n## Step 3: Define Your Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THESE to your variable names\npredictor <- \"predictor\"   # Your IV (continuous)\noutcome <- \"outcome\"       # Your DV (continuous)\n```\n:::\n\n\n\n## Step 4: Descriptive Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  summarise(\n    # Predictor\n    pred_mean = mean(.data[[predictor]], na.rm = TRUE),\n    pred_sd = sd(.data[[predictor]], na.rm = TRUE),\n    pred_min = min(.data[[predictor]], na.rm = TRUE),\n    pred_max = max(.data[[predictor]], na.rm = TRUE),\n    # Outcome\n    out_mean = mean(.data[[outcome]], na.rm = TRUE),\n    out_sd = sd(.data[[outcome]], na.rm = TRUE),\n    out_min = min(.data[[outcome]], na.rm = TRUE),\n    out_max = max(.data[[outcome]], na.rm = TRUE),\n    # Correlation\n    r = cor(.data[[predictor]], .data[[outcome]], use = \"complete.obs\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  pred_mean pred_sd pred_min pred_max out_mean out_sd out_min out_max     r\n      <dbl>   <dbl>    <dbl>    <dbl>    <dbl>  <dbl>   <dbl>   <dbl> <dbl>\n1      17.2    3.89       10       24     56.1   9.26      40      72 0.996\n```\n\n\n:::\n:::\n\n\n### Visualise: Scatterplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x = .data[[predictor]], y = .data[[outcome]])) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"steelblue\") +\n  labs(\n    x = \"Predictor\",\n    y = \"Outcome\",\n    title = \"Simple Linear Regression\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Relationship between predictor and outcome](07_simple_regression_files/figure-html/scatterplot-1.png){width=672}\n:::\n:::\n\n\n\n## Step 5: Run the Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(as.formula(paste(outcome, \"~\", predictor)), data = data)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = as.formula(paste(outcome, \"~\", predictor)), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.12692 -0.45495  0.02418  0.61264  1.24286 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15.31978    0.76216   20.10 4.34e-16 ***\npredictor    2.36978    0.04326   54.78  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8253 on 23 degrees of freedom\nMultiple R-squared:  0.9924,\tAdjusted R-squared:  0.9921 \nF-statistic:  3001 on 1 and 23 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Step 6: Extract Key Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_summary <- summary(model)\n\n# Coefficients\nintercept <- coef(model)[1]\nslope <- coef(model)[2]\n\n# R-squared\nr_squared <- model_summary$r.squared\nadj_r_squared <- model_summary$adj.r.squared\n\n# F-test\nf_stat <- model_summary$fstatistic[1]\ndf1 <- model_summary$fstatistic[2]\ndf2 <- model_summary$fstatistic[3]\nf_p <- pf(f_stat, df1, df2, lower.tail = FALSE)\n\n# Coefficient test\ncoef_table <- coef(model_summary)\nb <- coef_table[2, 1]\nse_b <- coef_table[2, 2]\nt_val <- coef_table[2, 3]\nt_p <- coef_table[2, 4]\n\ncat(\"Intercept (b0):\", round(intercept, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept (b0): 15.32 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Slope (b1):\", round(slope, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSlope (b1): 2.37 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R²:\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR²: 0.992 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adjusted R²:\", round(adj_r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted R²: 0.992 \n```\n\n\n:::\n:::\n\n\n\n## Step 7: Check Assumptions\n\n### 1. Linearity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 1)\n```\n\n::: {.cell-output-display}\n![Residuals vs Fitted values](07_simple_regression_files/figure-html/linearity-1.png){width=672}\n:::\n:::\n\n\n**Look for:** Random scatter around zero. Patterns suggest non-linearity.\n\n### 2. Normality of Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 2)\n```\n\n::: {.cell-output-display}\n![Q-Q plot of residuals](07_simple_regression_files/figure-html/normality-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Shapiro-Wilk test\nshapiro.test(residuals(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.96148, p-value = 0.4446\n```\n\n\n:::\n:::\n\n\n### 3. Homoscedasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 3)\n```\n\n::: {.cell-output-display}\n![Scale-Location plot](07_simple_regression_files/figure-html/homoscedasticity-1.png){width=672}\n:::\n:::\n\n\n**Look for:** Horizontal red line, evenly spread points.\n\n### 4. Influential Cases\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 4)\n```\n\n::: {.cell-output-display}\n![Cook's distance](07_simple_regression_files/figure-html/influential-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Cases with high Cook's D\nn <- nrow(data)\ninfluential <- which(cooks.distance(model) > 4/n)\nif(length(influential) > 0) {\n  cat(\"Potentially influential cases:\", influential, \"\\n\")\n} else {\n  cat(\"No highly influential cases detected\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPotentially influential cases: 1 14 24 \n```\n\n\n:::\n:::\n\n\n\n## Step 8: Confidence Intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 13.74313 16.89643\npredictor    2.28029  2.45927\n```\n\n\n:::\n:::\n\n\n\n## Step 9: Effect Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# f² from R²\nf_squared <- r_squared / (1 - r_squared)\n\ncat(\"R² =\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² = 0.992 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"f² =\", round(f_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nf² = 130.471 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation: f² = 0.02 (small), 0.15 (medium), 0.35 (large)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: f² = 0.02 (small), 0.15 (medium), 0.35 (large)\n```\n\n\n:::\n:::\n\n\n\n## Step 10: Summary of Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"=== SIMPLE REGRESSION RESULTS ===\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== SIMPLE REGRESSION RESULTS ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MODEL:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMODEL:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s = %.2f + %.2f × %s\\n\", outcome, intercept, slope, predictor))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  outcome = 15.32 + 2.37 × predictor\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMODEL FIT:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMODEL FIT:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  R² = %.3f (%.1f%% variance explained)\\n\", r_squared, r_squared * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.992 (99.2% variance explained)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  F(%d, %d) = %.2f, p = %.4f\\n\", df1, df2, f_stat, f_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  F(1, 23) = 3000.84, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nPREDICTOR:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPREDICTOR:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s: b = %.3f, SE = %.3f, t = %.2f, p = %.4f\\n\",\n            predictor, b, se_b, t_val, t_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor: b = 2.370, SE = 0.043, t = 54.78, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  95%% CI: [%.3f, %.3f]\\n\",\n            confint(model)[2, 1], confint(model)[2, 2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  95% CI: [2.280, 2.459]\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEFFECT SIZE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEFFECT SIZE:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  f² = %.3f\\n\", f_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  f² = 130.471\n```\n\n\n:::\n:::\n\n\n\n## Step 11: APA Write-Up Template\n\n::: {.callout-note}\n## APA Format\n\nA simple linear regression was conducted to predict [OUTCOME] from [PREDICTOR]. The model was [significant/non-significant], *F*(1, XX) = XX.XX, *p* = .XXX, explaining XX.X% of the variance in [OUTCOME] (*R*² = .XX).\n\n[PREDICTOR] was a [significant/non-significant] predictor of [OUTCOME], *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, 95% CI [X.XX, X.XX]. For each one-unit increase in [PREDICTOR], [OUTCOME] [increased/decreased] by X.XX units.\n:::\n\n\n## Checklist\n\n- [ ] Data loaded correctly\n- [ ] Variables defined\n- [ ] Scatterplot examined (linear relationship?)\n- [ ] Regression model run\n- [ ] Assumptions checked (linearity, normality, homoscedasticity)\n- [ ] Influential cases examined\n- [ ] Effect size calculated\n- [ ] Confidence intervals obtained\n- [ ] Results interpreted\n",
    "supporting": [
      "07_simple_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}