{
  "hash": "29c20a3571c463cbb4797908e48d411e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Boil in the Bag: Multiple Regression (3 Predictors)\"\nsubtitle: \"Three predictors, one outcome\"\nformat: html\n---\n\n## Overview\n\n**Use this template when:** You want to predict a continuous outcome from three continuous (or categorical) predictors.\n\n**Example scenarios:**\n- Predicting academic performance from motivation, ability, AND study time\n- Predicting wellbeing from sleep, exercise, AND social connection\n- Predicting lie detection accuracy from empathy, experience, AND anxiety\n\n\n## Step 1: Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(car)\n```\n:::\n\n\n\n## Step 2: Load Your Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THIS: Replace with your data file\ndata <- read_csv(\"data/multiple_regression_3pred_data.csv\")\n\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30\nColumns: 7\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ age        <dbl> 21, 22, 20, 23, 21, 22, 20, 24, 21, 22, 23, 20, 21, 22, 20,…\n$ gender     <dbl> 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1,…\n$ predictor1 <dbl> 12, 15, 18, 14, 20, 16, 22, 13, 19, 17, 21, 11, 24, 10, 23,…\n$ predictor2 <dbl> 5, 7, 6, 8, 9, 5, 8, 6, 7, 9, 6, 5, 10, 4, 8, 6, 7, 5, 9, 6…\n$ predictor3 <dbl> 3, 4, 5, 3, 6, 4, 5, 3, 5, 4, 6, 2, 6, 2, 5, 4, 5, 3, 5, 3,…\n$ outcome    <dbl> 48, 58, 65, 55, 75, 56, 78, 50, 68, 64, 73, 42, 85, 38, 80,…\n```\n\n\n:::\n:::\n\n\n\n## Step 3: Define Your Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THESE to your variable names\npredictor1 <- \"predictor1\"\npredictor2 <- \"predictor2\"\npredictor3 <- \"predictor3\"\noutcome <- \"outcome\"\n\nall_predictors <- c(predictor1, predictor2, predictor3)\n```\n:::\n\n\n\n## Step 4: Descriptive Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics\ndata |>\n  select(all_of(c(all_predictors, outcome))) |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   predictor1      predictor2       predictor3       outcome     \n Min.   :10.00   Min.   : 4.000   Min.   :2.000   Min.   :38.00  \n 1st Qu.:15.00   1st Qu.: 6.000   1st Qu.:3.000   1st Qu.:55.00  \n Median :17.50   Median : 7.000   Median :4.000   Median :64.50  \n Mean   :17.27   Mean   : 6.967   Mean   :4.267   Mean   :62.93  \n 3rd Qu.:20.00   3rd Qu.: 8.000   3rd Qu.:5.000   3rd Qu.:72.75  \n Max.   :24.00   Max.   :10.000   Max.   :6.000   Max.   :85.00  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Means and SDs table\ndata |>\n  summarise(\n    across(all_of(c(all_predictors, outcome)),\n           list(M = ~mean(., na.rm = TRUE),\n                SD = ~sd(., na.rm = TRUE)))\n  ) |>\n  pivot_longer(everything(),\n               names_to = c(\"variable\", \"stat\"),\n               names_sep = \"_\") |>\n  pivot_wider(names_from = stat, values_from = value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  variable       M    SD\n  <chr>      <dbl> <dbl>\n1 predictor1 17.3   3.63\n2 predictor2  6.97  1.56\n3 predictor3  4.27  1.20\n4 outcome    62.9  12.0 \n```\n\n\n:::\n:::\n\n\n### Correlation Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_matrix <- data |>\n  select(all_of(c(all_predictors, outcome))) |>\n  cor(use = \"complete.obs\")\n\nprint(round(cor_matrix, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           predictor1 predictor2 predictor3 outcome\npredictor1      1.000      0.694      0.916   0.989\npredictor2      0.694      1.000      0.574   0.765\npredictor3      0.916      0.574      1.000   0.926\noutcome         0.989      0.765      0.926   1.000\n```\n\n\n:::\n:::\n\n\n### Visualise Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple correlation heatmap\ncor_long <- as.data.frame(as.table(cor_matrix))\nnames(cor_long) <- c(\"Var1\", \"Var2\", \"r\")\n\nggplot(cor_long, aes(x = Var1, y = Var2, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = round(r, 2)), color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  labs(title = \"Correlation Matrix\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![Correlation matrix](09_multiple_regression_3pred_files/figure-html/corrplot-1.png){width=672}\n:::\n:::\n\n\n\n## Step 5: Run the Multiple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_str <- paste(outcome, \"~\", paste(all_predictors, collapse = \" + \"))\nmodel <- lm(as.formula(formula_str), data = data)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = as.formula(formula_str), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22161 -0.66392 -0.07005  0.52885  2.21170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.8154     0.8397   6.925 2.37e-07 ***\npredictor1    2.3031     0.1249  18.442  < 2e-16 ***\npredictor2    1.3365     0.1416   9.440 6.95e-10 ***\npredictor3    1.8843     0.3314   5.685 5.57e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8382 on 26 degrees of freedom\nMultiple R-squared:  0.9956,\tAdjusted R-squared:  0.9951 \nF-statistic:  1977 on 3 and 26 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Step 6: Extract Key Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_summary <- summary(model)\n\nr_squared <- model_summary$r.squared\nadj_r_squared <- model_summary$adj.r.squared\n\nf_stat <- model_summary$fstatistic[1]\ndf1 <- model_summary$fstatistic[2]\ndf2 <- model_summary$fstatistic[3]\nf_p <- pf(f_stat, df1, df2, lower.tail = FALSE)\n\ncat(\"Model Summary:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Summary:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  R² =\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.996 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Adjusted R² =\", round(adj_r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Adjusted R² = 0.995 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  F(%d, %d) = %.2f, p = %.4f\\n\", df1, df2, f_stat, f_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  F(3, 26) = 1977.38, p = 0.0000\n```\n\n\n:::\n:::\n\n\n\n## Step 7: Check Assumptions\n\n### 1. Linearity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 1)\n```\n\n::: {.cell-output-display}\n![Residuals vs Fitted](09_multiple_regression_3pred_files/figure-html/linearity-1.png){width=672}\n:::\n:::\n\n\n### 2. Normality of Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 2)\n```\n\n::: {.cell-output-display}\n![Q-Q plot](09_multiple_regression_3pred_files/figure-html/normality-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshapiro.test(residuals(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.95673, p-value = 0.2549\n```\n\n\n:::\n:::\n\n\n### 3. Homoscedasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 3)\n```\n\n::: {.cell-output-display}\n![Scale-Location plot](09_multiple_regression_3pred_files/figure-html/homoscedasticity-1.png){width=672}\n:::\n:::\n\n\n### 4. Multicollinearity (VIF)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif_values <- vif(model)\nprint(vif_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\npredictor1 predictor2 predictor3 \n  8.476836   2.024883   6.546735 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVIF Interpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVIF Interpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  < 5: OK\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  < 5: OK\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  5-10: Concerning\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  5-10: Concerning\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  > 10: Serious problem\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  > 10: Serious problem\n```\n\n\n:::\n\n```{.r .cell-code}\nif(any(vif_values > 5)) {\n  cat(\"\\n⚠️ WARNING: Some VIF values are concerning!\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n⚠️ WARNING: Some VIF values are concerning!\n```\n\n\n:::\n:::\n\n\n### 5. Influential Cases\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 4)\n```\n\n::: {.cell-output-display}\n![Cook's distance](09_multiple_regression_3pred_files/figure-html/influential-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Identify influential cases\nn <- nrow(data)\ninfluential <- which(cooks.distance(model) > 4/n)\nif(length(influential) > 0) {\n  cat(\"Potentially influential cases:\", influential, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPotentially influential cases: 1 7 15 18 \n```\n\n\n:::\n:::\n\n\n\n## Step 8: Confidence Intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 4.089275 7.541453\npredictor1  2.046429 2.559824\npredictor2  1.045465 1.627520\npredictor3  1.203015 2.565619\n```\n\n\n:::\n:::\n\n\n\n## Step 9: Standardised Coefficients (Beta Weights)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardise all variables\ndata_z <- data |>\n  mutate(across(all_of(c(all_predictors, outcome)), scale))\n\nmodel_std <- lm(as.formula(formula_str), data = data_z)\n\ncat(\"Standardised coefficients (β):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandardised coefficients (β):\n```\n\n\n:::\n\n```{.r .cell-code}\nround(coef(model_std), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)  predictor1  predictor2  predictor3 \n      0.000       0.696       0.174       0.188 \n```\n\n\n:::\n:::\n\n\n\n## Step 10: Unique Contribution of Each Predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate ΔR² for each predictor\ncat(\"Unique variance explained by each predictor:\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique variance explained by each predictor:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(pred in all_predictors) {\n  other_preds <- setdiff(all_predictors, pred)\n  formula_without <- paste(outcome, \"~\", paste(other_preds, collapse = \" + \"))\n  model_without <- lm(as.formula(formula_without), data = data)\n  delta_r2 <- r_squared - summary(model_without)$r.squared\n  cat(sprintf(\"  %s: ΔR² = %.4f (%.2f%%)\\n\", pred, delta_r2, delta_r2 * 100))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor1: ΔR² = 0.0571 (5.71%)\n  predictor2: ΔR² = 0.0150 (1.50%)\n  predictor3: ΔR² = 0.0054 (0.54%)\n```\n\n\n:::\n:::\n\n\n\n## Step 11: Effect Sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall f²\nf_squared <- r_squared / (1 - r_squared)\n\ncat(\"Overall model effect size:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOverall model effect size:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  R² =\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.996 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  f² =\", round(f_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  f² = 228.159 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation: f² = 0.02 (small), 0.15 (medium), 0.35 (large)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation: f² = 0.02 (small), 0.15 (medium), 0.35 (large)\n```\n\n\n:::\n:::\n\n\n\n## Step 12: Summary Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef_table <- coef(summary(model))\nci_table <- confint(model)\nstd_coefs <- coef(model_std)\n\n# Create summary table\nresults_table <- data.frame(\n  Predictor = rownames(coef_table),\n  b = coef_table[, 1],\n  SE = coef_table[, 2],\n  beta = std_coefs,\n  t = coef_table[, 3],\n  p = coef_table[, 4],\n  CI_lower = ci_table[, 1],\n  CI_upper = ci_table[, 2]\n)\n\n# Round for display\nresults_table |>\n  mutate(across(where(is.numeric), ~round(., 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Predictor     b    SE  beta      t p CI_lower CI_upper\n(Intercept) (Intercept) 5.815 0.840 0.000  6.925 0    4.089    7.541\npredictor1   predictor1 2.303 0.125 0.696 18.442 0    2.046    2.560\npredictor2   predictor2 1.336 0.142 0.174  9.440 0    1.045    1.628\npredictor3   predictor3 1.884 0.331 0.188  5.685 0    1.203    2.566\n```\n\n\n:::\n:::\n\n\n\n## Step 13: Full Results Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"=== MULTIPLE REGRESSION RESULTS (3 PREDICTORS) ===\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== MULTIPLE REGRESSION RESULTS (3 PREDICTORS) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MODEL FIT:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMODEL FIT:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  R² = %.3f\\n\", r_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.996\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Adjusted R² = %.3f\\n\", adj_r_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Adjusted R² = 0.995\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  F(%d, %d) = %.2f, p = %.4f\\n\\n\", df1, df2, f_stat, f_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  F(3, 26) = 1977.38, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"COEFFICIENTS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCOEFFICIENTS:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(i in 1:nrow(coef_table)) {\n  pred_name <- rownames(coef_table)[i]\n  cat(sprintf(\"  %s: b = %.3f, SE = %.3f, β = %.3f, t = %.2f, p = %.4f\\n\",\n              pred_name,\n              coef_table[i, 1], coef_table[i, 2],\n              std_coefs[i],\n              coef_table[i, 3], coef_table[i, 4]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept): b = 5.815, SE = 0.840, β = 0.000, t = 6.93, p = 0.0000\n  predictor1: b = 2.303, SE = 0.125, β = 0.696, t = 18.44, p = 0.0000\n  predictor2: b = 1.336, SE = 0.142, β = 0.174, t = 9.44, p = 0.0000\n  predictor3: b = 1.884, SE = 0.331, β = 0.188, t = 5.69, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMULTICOLLINEARITY:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMULTICOLLINEARITY:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(i in 1:length(vif_values)) {\n  cat(sprintf(\"  %s: VIF = %.2f\\n\", names(vif_values)[i], vif_values[i]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor1: VIF = 8.48\n  predictor2: VIF = 2.02\n  predictor3: VIF = 6.55\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nEFFECT SIZE:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEFFECT SIZE:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  f² = %.3f\\n\", f_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  f² = 228.159\n```\n\n\n:::\n:::\n\n\n\n## Step 14: APA Write-Up Template\n\n::: {.callout-note}\n## APA Format\n\nA multiple regression analysis was conducted to predict [OUTCOME] from [PREDICTOR 1], [PREDICTOR 2], and [PREDICTOR 3]. The overall model was [significant/non-significant], *F*(3, XX) = XX.XX, *p* = .XXX, accounting for XX.X% of the variance in [OUTCOME] (*R*² = .XX, adjusted *R*² = .XX).\n\n[PREDICTOR 1] was a [significant/non-significant] predictor of [OUTCOME], *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, β = X.XX, 95% CI [X.XX, X.XX].\n\n[PREDICTOR 2] was a [significant/non-significant] predictor, *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, β = X.XX, 95% CI [X.XX, X.XX].\n\n[PREDICTOR 3] was a [significant/non-significant] predictor, *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, β = X.XX, 95% CI [X.XX, X.XX].\n\nMulticollinearity diagnostics indicated no concerns (VIF values ranged from X.XX to X.XX).\n:::\n\n::: {.callout-note}\n## APA Format (Table)\n\nFor models with multiple predictors, a table is often clearer:\n\n**Table X**\n\n*Multiple Regression Results Predicting [OUTCOME]*\n\n| Predictor | *b* | *SE* | β | *t* | *p* | 95% CI |\n|-----------|-----|------|---|-----|-----|--------|\n| (Intercept) | XX.XX | X.XX | — | X.XX | .XXX | [X.XX, X.XX] |\n| [Predictor 1] | X.XX | X.XX | X.XX | X.XX | .XXX | [X.XX, X.XX] |\n| [Predictor 2] | X.XX | X.XX | X.XX | X.XX | .XXX | [X.XX, X.XX] |\n| [Predictor 3] | X.XX | X.XX | X.XX | X.XX | .XXX | [X.XX, X.XX] |\n\n*Note.* *R*² = .XX (adjusted *R*² = .XX). *F*(3, XX) = XX.XX, *p* = .XXX.\n:::\n\n\n## Checklist\n\n- [ ] Data loaded correctly\n- [ ] All three predictors defined\n- [ ] Correlations examined (especially between predictors)\n- [ ] Regression model run\n- [ ] Assumptions checked\n- [ ] Multicollinearity checked (all VIF < 5?)\n- [ ] Influential cases examined\n- [ ] Standardised coefficients calculated\n- [ ] Unique variance contribution calculated\n- [ ] Effect size calculated\n- [ ] Results interpreted\n",
    "supporting": [
      "09_multiple_regression_3pred_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}