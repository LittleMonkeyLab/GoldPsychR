{
  "hash": "214f2328a47f88e14e8f126a48b91617",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Boil in the Bag: Multiple Regression (2 Predictors)\"\nsubtitle: \"Two predictors, one outcome\"\nformat: html\n---\n\n## Overview\n\n**Use this template when:** You want to predict a continuous outcome from two continuous (or categorical) predictors.\n\n**Example scenarios:**\n- Predicting anxiety from social media use AND age\n- Predicting performance from motivation AND ability\n- Predicting wellbeing from sleep quality AND exercise\n\n\n## Step 1: Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(car)  # For VIF\n```\n:::\n\n\n\n## Step 2: Load Your Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THIS: Replace with your data file\ndata <- read_csv(\"data/multiple_regression_2pred_data.csv\")\n\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 25\nColumns: 6\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ age        <dbl> 21, 22, 20, 23, 21, 22, 20, 24, 21, 22, 23, 20, 21, 22, 20,…\n$ gender     <dbl> 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1,…\n$ predictor1 <dbl> 12, 15, 18, 14, 20, 16, 22, 13, 19, 17, 21, 11, 24, 10, 23,…\n$ predictor2 <dbl> 5, 7, 6, 8, 9, 5, 8, 6, 7, 9, 6, 5, 10, 4, 8, 6, 7, 5, 9, 6…\n$ outcome    <dbl> 45, 55, 60, 52, 68, 54, 72, 48, 62, 60, 67, 42, 78, 38, 74,…\n```\n\n\n:::\n:::\n\n\n\n## Step 3: Define Your Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CHANGE THESE to your variable names\npredictor1 <- \"predictor1\"   # First IV\npredictor2 <- \"predictor2\"   # Second IV\noutcome <- \"outcome\"         # DV\n```\n:::\n\n\n\n## Step 4: Descriptive Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics\ndata |>\n  select(all_of(c(predictor1, predictor2, outcome))) |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   predictor1     predictor2       outcome    \n Min.   :10.0   Min.   : 4.00   Min.   :38.0  \n 1st Qu.:14.0   1st Qu.: 6.00   1st Qu.:52.0  \n Median :17.0   Median : 7.00   Median :60.0  \n Mean   :17.2   Mean   : 6.92   Mean   :58.8  \n 3rd Qu.:20.0   3rd Qu.: 8.00   3rd Qu.:68.0  \n Max.   :24.0   Max.   :10.00   Max.   :78.0  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Means and SDs\ndata |>\n  summarise(\n    across(all_of(c(predictor1, predictor2, outcome)),\n           list(mean = ~mean(., na.rm = TRUE),\n                sd = ~sd(., na.rm = TRUE)))\n  ) |>\n  pivot_longer(everything(),\n               names_to = c(\"variable\", \"stat\"),\n               names_sep = \"_\") |>\n  pivot_wider(names_from = stat, values_from = value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  variable    mean    sd\n  <chr>      <dbl> <dbl>\n1 predictor1 17.2   3.89\n2 predictor2  6.92  1.63\n3 outcome    58.8  10.9 \n```\n\n\n:::\n:::\n\n\n### Correlation Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation matrix\ncor_matrix <- data |>\n  select(all_of(c(predictor1, predictor2, outcome))) |>\n  cor(use = \"complete.obs\")\n\nprint(round(cor_matrix, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           predictor1 predictor2 outcome\npredictor1      1.000      0.764   0.992\npredictor2      0.764      1.000   0.828\noutcome         0.992      0.828   1.000\n```\n\n\n:::\n:::\n\n\n### Visualise Relationships\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictor 1 vs Outcome\np1 <- ggplot(data, aes(x = .data[[predictor1]], y = .data[[outcome]])) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Predictor 1\", y = \"Outcome\") +\n  theme_minimal()\n\n# Predictor 2 vs Outcome\np2 <- ggplot(data, aes(x = .data[[predictor2]], y = .data[[outcome]])) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Predictor 2\", y = \"Outcome\") +\n  theme_minimal()\n\nlibrary(patchwork)\np1 + p2\n```\n\n::: {.cell-output-display}\n![Scatterplots of predictors with outcome](08_multiple_regression_2pred_files/figure-html/scatterplots-1.png){width=672}\n:::\n:::\n\n\n\n## Step 5: Run the Multiple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(as.formula(paste(outcome, \"~\", predictor1, \"+\", predictor2)),\n            data = data)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = as.formula(paste(outcome, \"~\", predictor1, \"+\", \n    predictor2)), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2321 -0.4581  0.1059  0.3158  1.4170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.52489    0.62962  15.128 4.14e-13 ***\npredictor1   2.41226    0.05304  45.480  < 2e-16 ***\npredictor2   1.12489    0.12665   8.882 9.97e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6534 on 22 degrees of freedom\nMultiple R-squared:  0.9967,\tAdjusted R-squared:  0.9964 \nF-statistic:  3315 on 2 and 22 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Step 6: Extract Key Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_summary <- summary(model)\n\n# Model fit\nr_squared <- model_summary$r.squared\nadj_r_squared <- model_summary$adj.r.squared\n\n# F-test\nf_stat <- model_summary$fstatistic[1]\ndf1 <- model_summary$fstatistic[2]\ndf2 <- model_summary$fstatistic[3]\nf_p <- pf(f_stat, df1, df2, lower.tail = FALSE)\n\ncat(\"Model R² =\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel R² = 0.997 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adjusted R² =\", round(adj_r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted R² = 0.996 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"F(%d, %d) = %.2f, p = %.4f\\n\", df1, df2, f_stat, f_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF(2, 22) = 3314.85, p = 0.0000\n```\n\n\n:::\n:::\n\n\n\n## Step 7: Check Assumptions\n\n### 1. Linearity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 1)\n```\n\n::: {.cell-output-display}\n![Residuals vs Fitted](08_multiple_regression_2pred_files/figure-html/linearity-1.png){width=672}\n:::\n:::\n\n\n### 2. Normality of Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 2)\n```\n\n::: {.cell-output-display}\n![Q-Q plot](08_multiple_regression_2pred_files/figure-html/normality-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshapiro.test(residuals(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98384, p-value = 0.9492\n```\n\n\n:::\n:::\n\n\n### 3. Homoscedasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 3)\n```\n\n::: {.cell-output-display}\n![Scale-Location plot](08_multiple_regression_2pred_files/figure-html/homoscedasticity-1.png){width=672}\n:::\n:::\n\n\n### 4. Multicollinearity (VIF)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif_values <- vif(model)\nprint(vif_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\npredictor1 predictor2 \n  2.398415   2.398415 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nInterpretation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nInterpretation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"VIF < 5: OK\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVIF < 5: OK\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"VIF 5-10: Concerning\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVIF 5-10: Concerning\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"VIF > 10: Serious multicollinearity\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVIF > 10: Serious multicollinearity\n```\n\n\n:::\n:::\n\n\n### 5. Influential Cases\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 4)\n```\n\n::: {.cell-output-display}\n![Cook's distance](08_multiple_regression_2pred_files/figure-html/influential-1.png){width=672}\n:::\n:::\n\n\n\n## Step 8: Confidence Intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %    97.5 %\n(Intercept) 8.2191332 10.830640\npredictor1  2.3022631  2.522260\npredictor2  0.8622283  1.387545\n```\n\n\n:::\n:::\n\n\n\n## Step 9: Standardised Coefficients (Beta Weights)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardise all variables\ndata_z <- data |>\n  mutate(across(all_of(c(predictor1, predictor2, outcome)), scale))\n\nmodel_std <- lm(as.formula(paste(outcome, \"~\", predictor1, \"+\", predictor2)),\n                data = data_z)\n\ncat(\"Standardised coefficients (β):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandardised coefficients (β):\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(model_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)    predictor1    predictor2 \n-7.372839e-17  8.636065e-01  1.686537e-01 \n```\n\n\n:::\n:::\n\n\n\n## Step 10: Effect Sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall f²\nf_squared <- r_squared / (1 - r_squared)\n\ncat(\"Overall model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOverall model:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  R² =\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.997 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  f² =\", round(f_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  f² = 301.35 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Unique contribution of each predictor (semi-partial correlations)\n# Can calculate by comparing R² of full model vs model without each predictor\nmodel_without_p1 <- lm(as.formula(paste(outcome, \"~\", predictor2)), data = data)\nmodel_without_p2 <- lm(as.formula(paste(outcome, \"~\", predictor1)), data = data)\n\ndelta_r2_p1 <- r_squared - summary(model_without_p1)$r.squared\ndelta_r2_p2 <- r_squared - summary(model_without_p2)$r.squared\n\ncat(\"\\nUnique variance explained:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nUnique variance explained:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s: ΔR² = %.3f\\n\", predictor1, delta_r2_p1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor1: ΔR² = 0.311\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s: ΔR² = %.3f\\n\", predictor2, delta_r2_p2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor2: ΔR² = 0.012\n```\n\n\n:::\n:::\n\n\n\n## Step 11: Summary of Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef_table <- coef(summary(model))\n\ncat(\"=== MULTIPLE REGRESSION RESULTS (2 PREDICTORS) ===\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== MULTIPLE REGRESSION RESULTS (2 PREDICTORS) ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MODEL FIT:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMODEL FIT:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  R² = %.3f, Adjusted R² = %.3f\\n\", r_squared, adj_r_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  R² = 0.997, Adjusted R² = 0.996\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  F(%d, %d) = %.2f, p = %.4f\\n\\n\", df1, df2, f_stat, f_p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  F(2, 22) = 3314.85, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"COEFFICIENTS:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCOEFFICIENTS:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  Intercept: b = %.3f, SE = %.3f\\n\",\n            coef_table[1, 1], coef_table[1, 2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Intercept: b = 9.525, SE = 0.630\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s: b = %.3f, SE = %.3f, t = %.2f, p = %.4f\\n\",\n            predictor1, coef_table[2, 1], coef_table[2, 2],\n            coef_table[2, 3], coef_table[2, 4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor1: b = 2.412, SE = 0.053, t = 45.48, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"  %s: b = %.3f, SE = %.3f, t = %.2f, p = %.4f\\n\",\n            predictor2, coef_table[3, 1], coef_table[3, 2],\n            coef_table[3, 3], coef_table[3, 4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  predictor2: b = 1.125, SE = 0.127, t = 8.88, p = 0.0000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nVIF values:\", round(vif_values, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVIF values: 2.4 2.4 \n```\n\n\n:::\n:::\n\n\n\n## Step 12: APA Write-Up Template\n\n::: {.callout-note}\n## APA Format\n\nA multiple regression was conducted to predict [OUTCOME] from [PREDICTOR 1] and [PREDICTOR 2]. The overall model was [significant/non-significant], *F*(2, XX) = XX.XX, *p* = .XXX, explaining XX.X% of the variance in [OUTCOME] (*R*² = .XX, adjusted *R*² = .XX).\n\n[PREDICTOR 1] was a [significant/non-significant] predictor, *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, β = X.XX.\n\n[PREDICTOR 2] was a [significant/non-significant] predictor, *b* = X.XX, *SE* = X.XX, *t*(XX) = X.XX, *p* = .XXX, β = X.XX.\n\nMulticollinearity was not a concern (VIF = X.XX and X.XX for [PREDICTOR 1] and [PREDICTOR 2], respectively).\n:::\n\n\n## Checklist\n\n- [ ] Data loaded correctly\n- [ ] Variables defined\n- [ ] Correlations examined\n- [ ] Regression model run\n- [ ] Assumptions checked (linearity, normality, homoscedasticity)\n- [ ] Multicollinearity checked (VIF)\n- [ ] Influential cases examined\n- [ ] Effect sizes calculated\n- [ ] Standardised coefficients obtained\n- [ ] Results interpreted\n",
    "supporting": [
      "08_multiple_regression_2pred_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}